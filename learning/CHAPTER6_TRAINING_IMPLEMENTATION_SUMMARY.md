# ç¬¬å…­ç« å¤§æ¨¡å‹è®­ç»ƒæµç¨‹å®è·µæ€»ç»“

> åŸºäºã€Šç¬¬å…­ç«  å¤§æ¨¡å‹è®­ç»ƒæµç¨‹å®è·µã€‹çš„å®Œæ•´è®­ç»ƒä½“ç³»å®ç°æŒ‡å—
> 
> **ä½œè€…**: shihom_wu  
> **ç‰ˆæœ¬**: 1.0.0  
> **å®Œæˆæ—¶é—´**: 2025-06-18

## ğŸ“‹ å®ç°æ¦‚è§ˆ

æœ¬ç« åŸºäº Transformers æ¡†æ¶ï¼Œå®ç°äº†å®Œæ•´çš„å¤§æ¨¡å‹è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š

- **æ¨¡å‹é¢„è®­ç»ƒ**: åŸºäº Transformers + DeepSpeed çš„åˆ†å¸ƒå¼é¢„è®­ç»ƒ
- **æœ‰ç›‘ç£å¾®è°ƒ(SFT)**: æŒ‡ä»¤å¾®è°ƒçš„å®Œæ•´å®ç°æµç¨‹
- **é«˜æ•ˆå¾®è°ƒ**: LoRA æŠ€æœ¯çš„åŸç†å’Œå®è·µåº”ç”¨
- **åå¥½å¯¹é½**: å¼ºåŒ–å­¦ä¹ å’Œå¥–åŠ±æ¨¡å‹åŸºç¡€

## ğŸš€ æ ¸å¿ƒæŠ€æœ¯æ ˆ

### ä¸»è¦æ¡†æ¶
- **Transformers**: HuggingFace çš„ç»Ÿä¸€æ¨¡å‹æ¡†æ¶
- **DeepSpeed**: å¾®è½¯çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶
- **PEFT**: é«˜æ•ˆå¾®è°ƒæ¡†æ¶ï¼Œæ”¯æŒ LoRA ç­‰æŠ€æœ¯
- **WandB**: è®­ç»ƒè¿‡ç¨‹ç›‘æ§å’Œå¯è§†åŒ–
- **Tokenizers**: é«˜æ•ˆçš„åˆ†è¯å™¨è®­ç»ƒå’Œä½¿ç”¨

### æŠ€æœ¯ç‰¹ç‚¹
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒæ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œã€ZeRO ä¼˜åŒ–
- **å†…å­˜ä¼˜åŒ–**: æ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦è®­ç»ƒ
- **é«˜æ•ˆå¾®è°ƒ**: LoRAã€Adapter ç­‰å‚æ•°é«˜æ•ˆæ–¹æ³•
- **è®­ç»ƒç›‘æ§**: å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œå¯è§†åŒ–

## ğŸ”§ æ¨¡å‹é¢„è®­ç»ƒå®ç°

### 1. æ¨¡å‹åˆå§‹åŒ–
```python
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

# ä»é…ç½®åˆå§‹åŒ–æ¨¡å‹
config = AutoConfig.from_pretrained(model_path)
model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)

# æˆ–ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    trust_remote_code=True
)

# åˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path)
```

### 2. æ•°æ®å¤„ç†æµç¨‹
```python
from datasets import load_dataset
from itertools import chain

# åŠ è½½æ•°æ®é›†
ds = load_dataset('json', data_files='data.jsonl')

# åˆ†è¯å¤„ç†
def tokenize_function(examples):
    return tokenizer([item for item in examples["text"]])

tokenized_datasets = ds.map(
    tokenize_function,
    batched=True,
    num_proc=10,
    remove_columns=column_names
)

# æ–‡æœ¬å—æ‹¼æ¥
def group_texts(examples):
    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    
    if total_length >= block_size:
        total_length = (total_length // block_size) * block_size
    
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```

### 3. åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
```bash
# DeepSpeed å¯åŠ¨è„šæœ¬
CUDA_VISIBLE_DEVICES=0,1,2,3

deepspeed pretrain.py \
    --config_name model_config_path \
    --tokenizer_name tokenizer_path \
    --train_files data.jsonl \
    --per_device_train_batch_size 16 \
    --gradient_accumulation_steps 4 \
    --learning_rate 1e-4 \
    --num_train_epochs 1 \
    --block_size 2048 \
    --bf16 \
    --gradient_checkpointing \
    --deepspeed ./ds_config_zero2.json \
    --report_to wandb
```

## ğŸ¯ æœ‰ç›‘ç£å¾®è°ƒ(SFT)å®ç°

### 1. Chat Template è®¾è®¡
```python
# ç‰¹æ®Š token å®šä¹‰
im_start = tokenizer("<|im_start|>").input_ids
im_end = tokenizer("<|im_end|>").input_ids
IGNORE_TOKEN_ID = tokenizer.pad_token_id

# è§’è‰²æ ‡è¯†ç¬¦
_system = tokenizer('system').input_ids + nl_tokens
_user = tokenizer('human').input_ids + nl_tokens
_assistant = tokenizer('assistant').input_ids + nl_tokens

# Qwen Chat Template
PROMPT_TEMPLATE = """å…ˆå¯¹ä¸Šä¸‹æ–‡è¿›è¡Œå†…å®¹æ€»ç»“,å†ä½¿ç”¨ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
é—®é¢˜: {question}
å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
Â·Â·Â·
{context}
Â·Â·Â·
æœ‰ç”¨çš„å›ç­”:"""
```

### 2. å¤šè½®å¯¹è¯æ•°æ®å¤„ç†
```python
def preprocess(sources, tokenizer, max_len):
    input_ids, targets = [], []
    
    for source in sources:
        input_id, target = [], []
        
        # æ·»åŠ  system æ¶ˆæ¯
        system = im_start + _system + tokenizer(system_message).input_ids + im_end + nl_tokens
        input_id += system
        target += im_start + [IGNORE_TOKEN_ID] * (len(system)-3) + im_end + nl_tokens
        
        # å¤„ç†å¤šè½®å¯¹è¯
        for sentence in source:
            role = roles[sentence["from"]]
            _input_id = tokenizer(role).input_ids + nl_tokens + \
                tokenizer(sentence["value"]).input_ids + im_end + nl_tokens
            input_id += _input_id
            
            if role == '<|im_start|>human':
                # user ä¸éœ€è¦æ‹Ÿåˆ
                _target = im_start + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + im_end + nl_tokens
            elif role == '<|im_start|>assistant':
                # assistant éœ€è¦æ‹Ÿåˆ
                _target = im_start + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \
                    _input_id[len(tokenizer(role).input_ids)+1:-2] + im_end + nl_tokens
            
            target += _target
        
        # å¡«å……åˆ°æœ€å¤§é•¿åº¦
        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))
        target += [IGNORE_TOKEN_ID] * (max_len - len(target))
        
        input_ids.append(input_id[:max_len])
        targets.append(target[:max_len])
    
    return dict(
        input_ids=torch.tensor(input_ids),
        labels=torch.tensor(targets),
        attention_mask=torch.tensor(input_ids).ne(tokenizer.pad_token_id)
    )
```

## âš¡ é«˜æ•ˆå¾®è°ƒ(LoRA)å®ç°

### 1. LoRA åŸç†
LoRA é€šè¿‡ä½ç§©åˆ†è§£æ¥è¿‘ä¼¼æƒé‡æ›´æ–°ï¼š
- **åŸç†**: $W_0 + \Delta W = W_0 + BA$ï¼Œå…¶ä¸­ $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$
- **å‰å‘ä¼ æ’­**: $h = W_0 x + \Delta W x = W_0 x + B A x$
- **å‚æ•°é‡**: $\Theta = 2 \times L_{LoRA} \times d_{model} \times r$

### 2. PEFT åº“ä½¿ç”¨
```python
from peft import get_peft_model, LoraConfig, TaskType

# LoRA é…ç½®
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,                    # LoRA ç§©
    lora_alpha=32,          # ç¼©æ”¾å‚æ•°
    lora_dropout=0.1,       # Dropout æ¯”ä¾‹
    target_modules=["q_proj", "v_proj"]  # ç›®æ ‡æ¨¡å—
)

# è·å– LoRA æ¨¡å‹
model = get_peft_model(model, peft_config)

# è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer
)
trainer.train()
```

### 3. LoRA å±‚å®ç°åŸç†
```python
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, r=8, lora_alpha=32):
        super().__init__()
        self.r = r
        self.lora_alpha = lora_alpha
        
        # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        self.weight.requires_grad = False
        
        # LoRA çŸ©é˜µ
        self.lora_A = nn.Linear(in_features, r, bias=False)
        self.lora_B = nn.Linear(r, out_features, bias=False)
        self.scaling = self.lora_alpha / self.r
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)
    
    def forward(self, x):
        # åŸå§‹å‰å‘ä¼ æ’­
        result = F.linear(x, self.weight)
        
        # LoRA å‰å‘ä¼ æ’­
        if self.r > 0:
            result += self.lora_B(self.lora_A(x)) * self.scaling
        
        return result
```

## ğŸ–ï¸ åå¥½å¯¹é½åŸºç¡€

### 1. å¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ
- **çŠ¶æ€(State)**: ç³»ç»Ÿå½“å‰çš„å…·ä½“çŠ¶å†µ
- **åŠ¨ä½œ(Action)**: æ™ºèƒ½ä½“å¯æ‰§è¡Œçš„æ“ä½œ
- **å¥–åŠ±(Reward)**: æ‰§è¡ŒåŠ¨ä½œåè·å¾—çš„åé¦ˆ
- **ç­–ç•¥(Policy)**: æŒ‡å¯¼åŠ¨ä½œé€‰æ‹©çš„è§„åˆ™
- **ä»·å€¼å‡½æ•°(Value Function)**: é¢„æµ‹é•¿æœŸå¥–åŠ±çš„å·¥å…·

### 2. å¥–åŠ±æ¨¡å‹è®­ç»ƒ
```python
# å¥–åŠ±æ¨¡å‹æ•°æ®æ ¼å¼
reward_data = [
    {
        "question": "Pythonä¸­çš„åˆ—è¡¨æ˜¯ä»€ä¹ˆï¼Ÿ",
        "chosen": "Pythonä¸­çš„åˆ—è¡¨æ˜¯ä¸€ç§æœ‰åºçš„å¯å˜å®¹å™¨ï¼Œå…è®¸å­˜å‚¨å¤šä¸ªå…ƒç´ ...",
        "rejected": "Pythonä¸­çš„åˆ—è¡¨ç”¨äºå­˜å‚¨æ•°æ®ã€‚"
    }
]

# ä½¿ç”¨ TRL æ¡†æ¶è®­ç»ƒå¥–åŠ±æ¨¡å‹
from trl import RewardTrainer

reward_trainer = RewardTrainer(
    model=reward_model,
    args=training_args,
    train_dataset=reward_dataset,
    tokenizer=tokenizer
)
reward_trainer.train()
```

## ğŸ“Š æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”

| æŠ€æœ¯ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | èµ„æºéœ€æ±‚ |
|------|------|----------|----------|
| **å…¨é‡å¾®è°ƒ** | æ•ˆæœæœ€å¥½ | å……è¶³èµ„æºã€é‡è¦ä»»åŠ¡ | é«˜ |
| **LoRAå¾®è°ƒ** | é«˜æ•ˆã€çµæ´» | èµ„æºå—é™ã€å¿«é€Ÿé€‚é… | ä¸­ |
| **Adapterå¾®è°ƒ** | æ¨¡å—åŒ– | å¤šä»»åŠ¡åˆ‡æ¢ | ä¸­ |
| **Prefixå¾®è°ƒ** | å‚æ•°å°‘ | è½»é‡åŒ–éƒ¨ç½² | ä½ |

## ğŸ”® å®è·µå»ºè®®

### é¢„è®­ç»ƒæœ€ä½³å®è·µ
1. **æ•°æ®è´¨é‡**: ç¡®ä¿è®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§
2. **è¶…å‚è°ƒä¼˜**: åˆç†è®¾ç½®å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ç­‰
3. **ç›‘æ§è®­ç»ƒ**: ä½¿ç”¨ WandB ç­‰å·¥å…·ç›‘æ§è®­ç»ƒè¿‡ç¨‹
4. **æ£€æŸ¥ç‚¹ç®¡ç†**: å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­

### SFT æœ€ä½³å®è·µ
1. **æ•°æ®æ ¼å¼**: ä¸¥æ ¼æŒ‰ç…§ Chat Template æ ¼å¼åŒ–æ•°æ®
2. **æ©ç ç­–ç•¥**: æ­£ç¡®è®¾ç½® IGNORE_TOKEN_ID
3. **é•¿åº¦æ§åˆ¶**: åˆç†è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
4. **è´¨é‡è¯„ä¼°**: å®šæœŸè¯„ä¼°æ¨¡å‹è¾“å‡ºè´¨é‡

### LoRA æœ€ä½³å®è·µ
1. **ç§©é€‰æ‹©**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„ç§©(4-16)
2. **ç›®æ ‡æ¨¡å—**: ä¼˜å…ˆé€‰æ‹©æ³¨æ„åŠ›å±‚çš„ q_proj å’Œ v_proj
3. **å­¦ä¹ ç‡**: LoRA é€šå¸¸éœ€è¦è¾ƒé«˜çš„å­¦ä¹ ç‡
4. **åˆå¹¶ç­–ç•¥**: è®­ç»ƒå®Œæˆåå¯é€‰æ‹©åˆå¹¶æƒé‡

## ğŸ† é¡¹ç›®æˆæœ

### å®ç°å®Œæ•´æ€§
- âœ… å®Œæ•´çš„é¢„è®­ç»ƒæµç¨‹
- âœ… æ ‡å‡†çš„ SFT å®ç°
- âœ… é«˜æ•ˆçš„ LoRA å¾®è°ƒ
- âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- âœ… å®Œæ•´çš„ç›‘æ§ä½“ç³»

### æŠ€æœ¯åˆ›æ–°
- ğŸ”§ æ¨¡å—åŒ–çš„è®­ç»ƒæ¡†æ¶
- ğŸ“Š å®Œæ•´çš„æ€§èƒ½ç›‘æ§
- ğŸš€ é«˜æ•ˆçš„å†…å­˜ç®¡ç†
- ğŸ¯ çµæ´»çš„é…ç½®ç³»ç»Ÿ

---

**æœ¬å®ç°ä¸ºã€Šç¬¬å…­ç«  å¤§æ¨¡å‹è®­ç»ƒæµç¨‹å®è·µã€‹çš„å®Œæ•´æŠ€æœ¯å®ç°ï¼Œæä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´è®­ç»ƒä½“ç³»ã€‚**
