# ç¬¬ä¸ƒç« å¤§æ¨¡å‹åº”ç”¨æŠ€æœ¯å®è·µæ€»ç»“

> åŸºäºã€Šç¬¬ä¸ƒç«  å¤§æ¨¡å‹åº”ç”¨ã€‹çš„ RAG å’Œ Agent æŠ€æœ¯å®Œæ•´å®ç°æŒ‡å—
> 
> **ä½œè€…**: shihom_wu  
> **ç‰ˆæœ¬**: 1.0.0  
> **å®Œæˆæ—¶é—´**: 2025-06-18

## ğŸ“‹ å®ç°æ¦‚è§ˆ

æœ¬ç« åŸºäºå¤§æ¨¡å‹åº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå®ç°äº†ä¸‰å¤§åº”ç”¨æ–¹å‘ï¼š

- **LLM è¯„æµ‹ä½“ç³»**: å…¨é¢çš„æ¨¡å‹è¯„æµ‹æ–¹æ³•å’Œæ¦œå•åˆ†æ
- **RAG æŠ€æœ¯**: æ£€ç´¢å¢å¼ºç”Ÿæˆçš„å®Œæ•´å®ç°æ¡†æ¶
- **Agent æŠ€æœ¯**: æ™ºèƒ½ä»£ç†çš„è®¾è®¡å’Œå®ç°æ–¹æ¡ˆ

## ğŸ¯ LLM è¯„æµ‹ä½“ç³»

### ä¸»æµè¯„æµ‹æ•°æ®é›†
| ç±»åˆ« | æ•°æ®é›† | è¯„æµ‹å†…å®¹ | åº”ç”¨åœºæ™¯ |
|------|--------|----------|----------|
| **é€šç”¨è¯„æµ‹** | MMLU | å¤šå­¦ç§‘ç†è§£èƒ½åŠ› | ç»¼åˆçŸ¥è¯†è¯„ä¼° |
| **å·¥å…·ä½¿ç”¨** | BFCL V2 | å¤æ‚å·¥å…·ä½¿ç”¨ | å¤šæ­¥éª¤æ“ä½œ |
| **æ•°å­¦æ¨ç†** | GSM8K | å°å­¦æ•°å­¦é—®é¢˜ | é€»è¾‘æ¨ç†èƒ½åŠ› |
| **ç§‘å­¦æ¨ç†** | ARC Challenge | ç§‘å­¦å¸¸è¯†æ¨ç† | ä¸“ä¸šçŸ¥è¯†åº”ç”¨ |
| **é•¿æ–‡æœ¬** | InfiniteBench | é•¿æ–‡æ¡£ç†è§£ | æ–‡æ¡£åˆ†æèƒ½åŠ› |
| **å¤šè¯­è¨€** | MGSM | å¤šè¯­è¨€æ•°å­¦ | è·¨è¯­è¨€é€‚åº”æ€§ |

### ä¸»æµè¯„æµ‹æ¦œå•
- **Open LLM Leaderboard**: HuggingFace å¼€æºæ¨¡å‹æ¦œå•
- **Lmsys Chatbot Arena**: å¯¹è¯èƒ½åŠ›è¯„æµ‹æ¦œå•
- **OpenCompass**: å›½å†…ç»¼åˆè¯„æµ‹å¹³å°
- **å‚ç›´é¢†åŸŸæ¦œå•**: é‡‘èã€æ³•å¾‹ã€åŒ»ç–—ç­‰ä¸“ä¸šé¢†åŸŸ

### è¯„æµ‹å®è·µå»ºè®®
```python
# è¯„æµ‹æ¡†æ¶ç¤ºä¾‹
class LLMEvaluator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def evaluate_dataset(self, dataset_name, test_data):
        """è¯„æµ‹æŒ‡å®šæ•°æ®é›†"""
        results = []
        for sample in test_data:
            prediction = self.model.generate(sample['input'])
            score = self.calculate_score(prediction, sample['target'])
            results.append(score)
        return np.mean(results)
    
    def calculate_score(self, prediction, target):
        """è®¡ç®—è¯„æµ‹åˆ†æ•°"""
        # æ ¹æ®ä¸åŒä»»åŠ¡ç±»å‹è®¡ç®—åˆ†æ•°
        pass
```

## ğŸ” RAG æŠ€æœ¯å®ç°

### 1. RAG æ¶æ„è®¾è®¡
```
ç”¨æˆ·æŸ¥è¯¢ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ LLMç”Ÿæˆ â†’ è¿”å›ç»“æœ
```

### 2. æ ¸å¿ƒç»„ä»¶å®ç°

#### å‘é‡åŒ–æ¨¡å—
```python
class BaseEmbeddings:
    """å‘é‡åŒ–åŸºç±»"""
    def __init__(self, path: str, is_api: bool) -> None:
        self.path = path
        self.is_api = is_api
    
    def get_embedding(self, text: str, model: str) -> List[float]:
        """è·å–æ–‡æœ¬å‘é‡è¡¨ç¤º"""
        raise NotImplementedError
    
    @classmethod
    def cosine_similarity(cls, vector1: List[float], vector2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot_product = np.dot(vector1, vector2)
        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)
        if not magnitude:
            return 0
        return dot_product / magnitude

class OpenAIEmbedding(BaseEmbeddings):
    """OpenAI å‘é‡åŒ–å®ç°"""
    def get_embedding(self, text: str, model: str = "text-embedding-3-large") -> List[float]:
        if self.is_api:
            text = text.replace("\n", " ")
            return self.client.embeddings.create(input=[text], model=model).data[0].embedding
```

#### æ–‡æ¡£å¤„ç†æ¨¡å—
```python
class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    @classmethod
    def read_file_content(cls, file_path: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹"""
        if file_path.endswith('.pdf'):
            return cls.read_pdf(file_path)
        elif file_path.endswith('.md'):
            return cls.read_markdown(file_path)
        elif file_path.endswith('.txt'):
            return cls.read_text(file_path)
        else:
            raise ValueError("Unsupported file type")
    
    @classmethod
    def get_chunk(cls, text: str, max_token_len: int = 600, cover_content: int = 150):
        """æ–‡æ¡£åˆ†å—å¤„ç†"""
        chunk_text = []
        curr_len = 0
        curr_chunk = ''
        
        lines = text.split('\n')
        for line in lines:
            line_len = len(enc.encode(line))
            if curr_len + line_len <= max_token_len:
                curr_chunk += line + '\n'
                curr_len += line_len + 1
            else:
                chunk_text.append(curr_chunk)
                curr_chunk = curr_chunk[-cover_content:] + line
                curr_len = line_len + cover_content
        
        if curr_chunk:
            chunk_text.append(curr_chunk)
        
        return chunk_text
```

#### å‘é‡æ•°æ®åº“
```python
class VectorStore:
    """å‘é‡æ•°æ®åº“"""
    def __init__(self, document: List[str] = ['']):
        self.document = document
        self.vectors = []
    
    def get_vector(self, EmbeddingModel: BaseEmbeddings) -> List[List[float]]:
        """è·å–æ–‡æ¡£å‘é‡è¡¨ç¤º"""
        self.vectors = []
        for doc in self.document:
            vector = EmbeddingModel.get_embedding(doc)
            self.vectors.append(vector)
        return self.vectors
    
    def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        query_vector = EmbeddingModel.get_embedding(query)
        similarities = []
        
        for vector in self.vectors:
            similarity = EmbeddingModel.cosine_similarity(query_vector, vector)
            similarities.append(similarity)
        
        # è¿”å›æœ€ç›¸ä¼¼çš„ k ä¸ªæ–‡æ¡£
        indices = np.argsort(similarities)[-k:][::-1]
        return [self.document[i] for i in indices]
    
    def persist(self, path: str = 'storage'):
        """æŒä¹…åŒ–ä¿å­˜"""
        import pickle
        with open(f'{path}/vectors.pkl', 'wb') as f:
            pickle.dump(self.vectors, f)
        with open(f'{path}/documents.pkl', 'wb') as f:
            pickle.dump(self.document, f)
    
    def load_vector(self, path: str = 'storage'):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        import pickle
        with open(f'{path}/vectors.pkl', 'rb') as f:
            self.vectors = pickle.load(f)
        with open(f'{path}/documents.pkl', 'rb') as f:
            self.document = pickle.load(f)
```

#### LLM æ¨¡å—
```python
class BaseModel:
    """LLM åŸºç±»"""
    def __init__(self, path: str = ''):
        self.path = path
    
    def chat(self, prompt: str, history: List[dict], content: str) -> str:
        """å¯¹è¯æ¥å£"""
        pass
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        pass

class InternLMChat(BaseModel):
    """InternLM å¯¹è¯æ¨¡å‹"""
    def chat(self, prompt: str, history: List = [], content: str = '') -> str:
        prompt_template = """å…ˆå¯¹ä¸Šä¸‹æ–‡è¿›è¡Œå†…å®¹æ€»ç»“,å†ä½¿ç”¨ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        é—®é¢˜: {question}
        å¯å‚è€ƒçš„ä¸Šä¸‹æ–‡ï¼š
        Â·Â·Â·
        {context}
        Â·Â·Â·
        æœ‰ç”¨çš„å›ç­”:"""
        
        formatted_prompt = prompt_template.format(question=prompt, context=content)
        response, history = self.model.chat(self.tokenizer, formatted_prompt, history)
        return response
```

### 3. Tiny-RAG å®Œæ•´ç¤ºä¾‹
```python
from RAG.VectorBase import VectorStore
from RAG.utils import ReadFiles
from RAG.LLM import InternLMChat
from RAG.Embeddings import ZhipuEmbedding

# æ„å»º RAG ç³»ç»Ÿ
docs = ReadFiles('./data').get_content(max_token_len=600, cover_content=150)
vector = VectorStore(docs)
embedding = ZhipuEmbedding()
vector.get_vector(EmbeddingModel=embedding)
vector.persist(path='storage')

# ä½¿ç”¨ RAG å›ç­”é—®é¢˜
question = 'gitçš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ'
content = vector.query(question, EmbeddingModel=embedding, k=1)[0]
chat = InternLMChat(path='model_path')
answer = chat.chat(question, [], content)
print(answer)
```

## ğŸ¤– Agent æŠ€æœ¯å®ç°

### 1. Agent æ¶æ„è®¾è®¡
```
ç”¨æˆ·è¾“å…¥ â†’ æ„å›¾ç†è§£ â†’ ä»»åŠ¡è§„åˆ’ â†’ å·¥å…·è°ƒç”¨ â†’ ç»“æœæ•´åˆ â†’ è¿”å›å“åº”
```

### 2. æ ¸å¿ƒç»„ä»¶å®ç°

#### å·¥å…·å‡½æ•°å®šä¹‰
```python
# src/tools.py
from datetime import datetime

def get_current_datetime() -> str:
    """
    è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´ã€‚
    :return: å½“å‰æ—¥æœŸå’Œæ—¶é—´çš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚
    """
    current_datetime = datetime.now()
    return current_datetime.strftime("%Y-%m-%d %H:%M:%S")

def add(a: float, b: float) -> str:
    """
    è®¡ç®—ä¸¤ä¸ªæµ®ç‚¹æ•°çš„å’Œã€‚
    :param a: ç¬¬ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚
    :param b: ç¬¬äºŒä¸ªæµ®ç‚¹æ•°ã€‚
    :return: ä¸¤ä¸ªæµ®ç‚¹æ•°çš„å’Œã€‚
    """
    return str(a + b)

def compare(a: float, b: float) -> str:
    """
    æ¯”è¾ƒä¸¤ä¸ªæµ®ç‚¹æ•°çš„å¤§å°ã€‚
    :param a: ç¬¬ä¸€ä¸ªæµ®ç‚¹æ•°ã€‚
    :param b: ç¬¬äºŒä¸ªæµ®ç‚¹æ•°ã€‚
    :return: æ¯”è¾ƒç»“æœçš„å­—ç¬¦ä¸²è¡¨ç¤ºã€‚
    """
    if a > b:
        return f'{a} is greater than {b}'
    elif a < b:
        return f'{b} is greater than {a}'
    else:
        return f'{a} is equal to {b}'
```

#### Agent æ ¸å¿ƒç±»
```python
class Agent:
    """æ™ºèƒ½ä»£ç†æ ¸å¿ƒç±»"""
    def __init__(self, client: OpenAI, model: str, tools: List, verbose: bool = True):
        self.client = client
        self.tools = tools
        self.model = model
        self.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·æ¥å›ç­”é—®é¢˜ã€‚"},
        ]
        self.verbose = verbose
    
    def get_tool_schema(self) -> List[Dict[str, Any]]:
        """è·å–å·¥å…·çš„ JSON Schema"""
        return [function_to_json(tool) for tool in self.tools]
    
    def handle_tool_call(self, tool_call):
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        function_name = tool_call.function.name
        function_args = tool_call.function.arguments
        function_id = tool_call.id
        
        # æ‰§è¡Œå·¥å…·å‡½æ•°
        function_call_content = eval(f"{function_name}(**{function_args})")
        
        return {
            "role": "tool",
            "content": function_call_content,
            "tool_call_id": function_id,
        }
    
    def get_completion(self, prompt) -> str:
        """è·å– Agent å“åº”"""
        self.messages.append({"role": "user", "content": prompt})
        
        # è°ƒç”¨ LLM
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            tools=self.get_tool_schema(),
            stream=False,
        )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
        if response.choices[0].message.tool_calls:
            self.messages.append({"role": "assistant", "content": response.choices[0].message.content})
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            for tool_call in response.choices[0].message.tool_calls:
                tool_result = self.handle_tool_call(tool_call)
                self.messages.append(tool_result)
                
                if self.verbose:
                    print(f"è°ƒç”¨å·¥å…·: {tool_call.function.name}")
            
            # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
            response = self.client.chat.completions.create(
                model=self.model,
                messages=self.messages,
                tools=self.get_tool_schema(),
                stream=False,
            )
        
        # ä¿å­˜ LLM å›å¤
        self.messages.append({"role": "assistant", "content": response.choices[0].message.content})
        return response.choices[0].message.content
```

### 3. Tiny-Agent å®Œæ•´ç¤ºä¾‹
```python
from openai import OpenAI
from src.core import Agent
from src.tools import get_current_datetime, add, compare, count_letter_in_string

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.siliconflow.cn/v1",
)

# åˆ›å»º Agent
agent = Agent(
    client=client,
    model="Qwen/Qwen2.5-32B-Instruct",
    tools=[get_current_datetime, add, compare, count_letter_in_string],
    verbose=True
)

# äº¤äº’å¼å¯¹è¯
while True:
    prompt = input("User: ")
    if prompt.lower() == "exit":
        break
    response = agent.get_completion(prompt)
    print("Assistant:", response)
```

## ğŸ“Š æŠ€æœ¯ç‰¹æ€§å¯¹æ¯”

| æŠ€æœ¯ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ | å®ç°å¤æ‚åº¦ |
|------|------|----------|------------|
| **RAG** | çŸ¥è¯†æ›´æ–°å¿«ã€å¯è¿½æº¯ | çŸ¥è¯†é—®ç­”ã€æ–‡æ¡£åˆ†æ | ä¸­ç­‰ |
| **Agent** | è‡ªä¸»è§„åˆ’ã€å·¥å…·ä½¿ç”¨ | å¤æ‚ä»»åŠ¡ã€è‡ªåŠ¨åŒ– | é«˜ |
| **å¾®è°ƒ** | æ·±åº¦å®šåˆ¶ã€æ€§èƒ½å¥½ | ç‰¹å®šé¢†åŸŸã€é«˜è´¨é‡è¦æ±‚ | é«˜ |
| **Promptå·¥ç¨‹** | ç®€å•å¿«é€Ÿã€æˆæœ¬ä½ | å¿«é€ŸåŸå‹ã€ç®€å•ä»»åŠ¡ | ä½ |

## ğŸ”® åº”ç”¨åœºæ™¯åˆ†æ

### RAG é€‚ç”¨åœºæ™¯
- **ä¼ä¸šçŸ¥è¯†åº“**: å†…éƒ¨æ–‡æ¡£æ£€ç´¢å’Œé—®ç­”
- **å®¢æˆ·æœåŠ¡**: åŸºäºäº§å“æ–‡æ¡£çš„æ™ºèƒ½å®¢æœ
- **å­¦æœ¯ç ”ç©¶**: è®ºæ–‡æ£€ç´¢å’Œæ–‡çŒ®åˆ†æ
- **æ³•å¾‹å’¨è¯¢**: æ³•è§„æ¡æ–‡æ£€ç´¢å’Œè§£é‡Š

### Agent é€‚ç”¨åœºæ™¯
- **ä»»åŠ¡è‡ªåŠ¨åŒ–**: å¤æ‚ä¸šåŠ¡æµç¨‹è‡ªåŠ¨åŒ–
- **æ™ºèƒ½åŠ©æ‰‹**: å¤šåŠŸèƒ½ä¸ªäººæˆ–ä¼ä¸šåŠ©æ‰‹
- **æ•°æ®åˆ†æ**: è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†å’Œåˆ†æ
- **è½¯ä»¶å¼€å‘**: ä»£ç ç”Ÿæˆå’Œè°ƒè¯•è¾…åŠ©

## ğŸ† å®è·µå»ºè®®

### RAG æœ€ä½³å®è·µ
1. **æ–‡æ¡£è´¨é‡**: ç¡®ä¿çŸ¥è¯†åº“æ–‡æ¡£çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
2. **åˆ†å—ç­–ç•¥**: åˆç†è®¾ç½®æ–‡æ¡£åˆ†å—å¤§å°å’Œé‡å åº¦
3. **å‘é‡æ¨¡å‹**: é€‰æ‹©é€‚åˆé¢†åŸŸçš„å‘é‡åŒ–æ¨¡å‹
4. **æ£€ç´¢ä¼˜åŒ–**: è°ƒæ•´æ£€ç´¢æ•°é‡å’Œç›¸ä¼¼åº¦é˜ˆå€¼

### Agent æœ€ä½³å®è·µ
1. **å·¥å…·è®¾è®¡**: è®¾è®¡æ¸…æ™°ã€åŠŸèƒ½å•ä¸€çš„å·¥å…·å‡½æ•°
2. **æç¤ºå·¥ç¨‹**: ä¼˜åŒ–ç³»ç»Ÿæç¤ºè¯å’Œå·¥å…·æè¿°
3. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. **å®‰å…¨æ§åˆ¶**: é™åˆ¶å·¥å…·æƒé™å’Œæ‰§è¡ŒèŒƒå›´

---

**æœ¬å®ç°ä¸ºã€Šç¬¬ä¸ƒç«  å¤§æ¨¡å‹åº”ç”¨ã€‹çš„å®Œæ•´æŠ€æœ¯å®ç°ï¼Œæä¾›äº†ä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´åº”ç”¨å¼€å‘æŒ‡å—ã€‚**
