"""
Transformer-PyTorch åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å„ä¸ªç»„ä»¶å’Œå®Œæ•´æ¨¡å‹ã€‚

ä½œè€…: Transformer-PyTorch Team
ç‰ˆæœ¬: 1.0.0
"""

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# å¯¼å…¥ Transformer-PyTorch ç»„ä»¶
from transformer_pytorch import (
    TransformerConfig,
    Transformer,
    get_config,
    create_config,
    print_config,
    get_device,
    set_seed,
    print_model_info,
)
from transformer_pytorch.core import (
    MultiHeadAttention,
    SelfAttention,
    TransformerEmbedding,
    create_encoder,
    create_decoder,
)


def example1_basic_components():
    """ç¤ºä¾‹1ï¼šåŸºç¡€ç»„ä»¶ä½¿ç”¨"""
    print("=== ç¤ºä¾‹1ï¼šåŸºç¡€ç»„ä»¶ä½¿ç”¨ ===")
    
    # è®¾ç½®éšæœºç§å­
    set_seed(42)
    
    # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
    print("æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶...")
    d_model, num_heads = 64, 4
    attention = MultiHeadAttention(d_model, num_heads, dropout=0.0)
    
    # åˆ›å»ºè¾“å…¥æ•°æ®
    batch_size, seq_len = 2, 8
    x = torch.randn(batch_size, seq_len, d_model)
    
    # å‰å‘ä¼ æ’­
    output, attention_weights = attention(x, x, x)
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"âœ… æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"âœ… å‚æ•°æ•°é‡: {sum(p.numel() for p in attention.parameters()):,}")
    
    # æµ‹è¯•åµŒå…¥å±‚
    print("\næµ‹è¯•åµŒå…¥å±‚...")
    vocab_size, max_len = 1000, 32
    embedding = TransformerEmbedding(vocab_size, d_model, max_len, dropout=0.0)
    
    token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    embedded = embedding(token_ids)
    
    print(f"âœ… è¯å…ƒIDå½¢çŠ¶: {token_ids.shape}")
    print(f"âœ… åµŒå…¥è¾“å‡ºå½¢çŠ¶: {embedded.shape}")
    print(f"âœ… åµŒå…¥å±‚å‚æ•°æ•°é‡: {sum(p.numel() for p in embedding.parameters()):,}")
    
    print()


def example2_model_configurations():
    """ç¤ºä¾‹2ï¼šæ¨¡å‹é…ç½®"""
    print("=== ç¤ºä¾‹2ï¼šæ¨¡å‹é…ç½® ===")
    
    # é¢„è®¾é…ç½®
    configs = ['small', 'default', 'large']
    
    for config_name in configs:
        print(f"\n--- {config_name.upper()} é…ç½® ---")
        config = get_config(config_name)
        
        print(f"è¯æ±‡è¡¨å¤§å°: {config.vocab_size:,}")
        print(f"æ¨¡å‹ç»´åº¦: {config.d_model}")
        print(f"ç¼–ç å™¨å±‚æ•°: {config.num_encoder_layers}")
        print(f"è§£ç å™¨å±‚æ•°: {config.num_decoder_layers}")
        print(f"æ³¨æ„åŠ›å¤´æ•°: {config.num_heads}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {config.max_seq_len}")
        
        # å‚æ•°ä¼°ç®—
        params = config.estimate_parameters()
        print(f"ä¼°ç®—å‚æ•°é‡: {params['total_M']}")
    
    # è‡ªå®šä¹‰é…ç½®
    print("\n--- è‡ªå®šä¹‰é…ç½® ---")
    custom_config = create_config(
        base_config=get_config('default'),
        vocab_size=8000,
        d_model=256,
        num_encoder_layers=4,
        num_decoder_layers=4,
        activation='gelu',
        dropout=0.05
    )
    
    print_config(custom_config)
    print()


def example3_complete_model():
    """ç¤ºä¾‹3ï¼šå®Œæ•´æ¨¡å‹ä½¿ç”¨"""
    print("=== ç¤ºä¾‹3ï¼šå®Œæ•´æ¨¡å‹ä½¿ç”¨ ===")
    
    # åˆ›å»ºå°å‹æ¨¡å‹ç”¨äºæ¼”ç¤º
    config = TransformerConfig(
        vocab_size=1000,
        d_model=128,
        num_heads=4,
        num_encoder_layers=3,
        num_decoder_layers=3,
        d_ff=256,
        max_seq_len=32,
        dropout=0.1
    )
    
    print("åˆ›å»º Transformer æ¨¡å‹...")
    model = Transformer(config)
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print_model_info(model)
    
    # å‡†å¤‡ç¤ºä¾‹æ•°æ®
    batch_size = 2
    src_len, tgt_len = 10, 8
    
    src = torch.randint(1, config.vocab_size, (batch_size, src_len))
    tgt = torch.randint(1, config.vocab_size, (batch_size, tgt_len))
    
    print(f"\nè¾“å…¥æ•°æ®:")
    print(f"æºåºåˆ—å½¢çŠ¶: {src.shape}")
    print(f"ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")
    
    # å‰å‘ä¼ æ’­
    print("\næ‰§è¡Œå‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        output = model(src, tgt)
    
    print(f"âœ… è¾“å‡º logits å½¢çŠ¶: {output['logits'].shape}")
    print(f"âœ… æœ€åéšè—çŠ¶æ€å½¢çŠ¶: {output['last_hidden_state'].shape}")
    
    # è®¡ç®—å›°æƒ‘åº¦
    logits = output['logits']
    # ç§»ä½æ ‡ç­¾ç”¨äºè¯­è¨€å»ºæ¨¡æŸå¤±
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = tgt[..., 1:].contiguous()
    
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=config.pad_token_id
    )
    perplexity = torch.exp(loss)
    
    print(f"âœ… æŸå¤±: {loss.item():.4f}")
    print(f"âœ… å›°æƒ‘åº¦: {perplexity.item():.4f}")
    
    print()


def example4_encoder_decoder_separate():
    """ç¤ºä¾‹4ï¼šç¼–ç å™¨å’Œè§£ç å™¨åˆ†åˆ«ä½¿ç”¨"""
    print("=== ç¤ºä¾‹4ï¼šç¼–ç å™¨å’Œè§£ç å™¨åˆ†åˆ«ä½¿ç”¨ ===")
    
    config = get_config('small')
    model = Transformer(config)
    model.eval()
    
    # å‡†å¤‡æ•°æ®
    src = torch.randint(1, config.vocab_size, (1, 12))
    tgt = torch.randint(1, config.vocab_size, (1, 8))
    
    print("åˆ†æ­¥æ‰§è¡Œç¼–ç -è§£ç ...")
    
    with torch.no_grad():
        # 1. ä»…ç¼–ç 
        print("1. ç¼–ç æºåºåˆ—...")
        encoder_output = model.encode(src)
        memory = encoder_output['last_hidden_state']
        print(f"   ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶: {memory.shape}")
        
        # 2. ä»…è§£ç 
        print("2. è§£ç ç›®æ ‡åºåˆ—...")
        decoder_output = model.decode(tgt, memory)
        logits = decoder_output['logits']
        print(f"   è§£ç å™¨è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        
        # 3. é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒ
        print("3. é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒ...")
        last_logits = logits[0, -1, :]  # æœ€åä¸€ä¸ªä½ç½®çš„ logits
        probs = F.softmax(last_logits, dim=-1)
        
        # Top-5 é¢„æµ‹
        top_probs, top_indices = torch.topk(probs, 5)
        print("   Top-5 é¢„æµ‹:")
        for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
            print(f"     {i+1}. è¯å…ƒ {idx.item()}: {prob.item():.4f}")
    
    print()


def example5_text_generation():
    """ç¤ºä¾‹5ï¼šæ–‡æœ¬ç”Ÿæˆ"""
    print("=== ç¤ºä¾‹5ï¼šæ–‡æœ¬ç”Ÿæˆ ===")
    
    # ä½¿ç”¨å°å‹é…ç½®
    config = TransformerConfig(
        vocab_size=500,
        d_model=64,
        num_heads=4,
        num_encoder_layers=2,
        num_decoder_layers=2,
        d_ff=128,
        max_seq_len=20
    )
    
    model = Transformer(config)
    model.eval()
    
    # æºåºåˆ—ï¼ˆç”¨äºæ¡ä»¶ç”Ÿæˆï¼‰
    src = torch.tensor([[config.bos_token_id, 10, 20, 30, config.eos_token_id]])
    
    print(f"æºåºåˆ—: {src.tolist()[0]}")
    
    # ç”Ÿæˆæ–‡æœ¬
    print("\nå¼€å§‹ç”Ÿæˆ...")
    with torch.no_grad():
        generated = model.generate(
            src=src,
            max_length=15,
            temperature=0.8,
            do_sample=True
        )
    
    print(f"ç”Ÿæˆåºåˆ—: {generated.tolist()[0]}")
    print(f"ç”Ÿæˆé•¿åº¦: {generated.size(1)}")
    
    print()


def example6_attention_visualization():
    """ç¤ºä¾‹6ï¼šæ³¨æ„åŠ›å¯è§†åŒ–"""
    print("=== ç¤ºä¾‹6ï¼šæ³¨æ„åŠ›å¯è§†åŒ– ===")
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    config = TransformerConfig(
        vocab_size=100,
        d_model=32,
        num_heads=2,
        num_encoder_layers=1,
        num_decoder_layers=1,
        d_ff=64,
        max_seq_len=10
    )
    
    model = Transformer(config)
    model.eval()
    
    # å‡†å¤‡æ•°æ®
    src = torch.tensor([[1, 5, 10, 15, 2]])  # åŒ…å« BOS å’Œ EOS
    tgt = torch.tensor([[1, 8, 12]])         # ç›®æ ‡åºåˆ—
    
    print(f"æºåºåˆ—: {src.tolist()[0]}")
    print(f"ç›®æ ‡åºåˆ—: {tgt.tolist()[0]}")
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    with torch.no_grad():
        output = model(src, tgt, return_dict=True)
    
    # å¯è§†åŒ–ç¼–ç å™¨æ³¨æ„åŠ›
    if output['encoder_attentions'] is not None:
        encoder_attn = output['encoder_attentions'][0]  # ç¬¬ä¸€å±‚
        head_0_attn = encoder_attn[0, 0].numpy()  # ç¬¬ä¸€ä¸ªå¤´
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            head_0_attn,
            annot=True,
            fmt='.3f',
            cmap='Blues',
            xticklabels=[f'Pos{i}' for i in range(head_0_attn.shape[1])],
            yticklabels=[f'Pos{i}' for i in range(head_0_attn.shape[0])]
        )
        plt.title('ç¼–ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡ (ç¬¬1å±‚, ç¬¬1å¤´)')
        plt.xlabel('é”®ä½ç½®')
        plt.ylabel('æŸ¥è¯¢ä½ç½®')
        plt.tight_layout()
        plt.savefig('encoder_attention.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("âœ… ç¼–ç å™¨æ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜ä¸º 'encoder_attention.png'")
    
    # å¯è§†åŒ–è§£ç å™¨è‡ªæ³¨æ„åŠ›
    if output['decoder_self_attentions'] is not None:
        decoder_self_attn = output['decoder_self_attentions'][0]  # ç¬¬ä¸€å±‚
        head_0_attn = decoder_self_attn[0, 0].numpy()  # ç¬¬ä¸€ä¸ªå¤´
        
        plt.figure(figsize=(6, 6))
        sns.heatmap(
            head_0_attn,
            annot=True,
            fmt='.3f',
            cmap='Reds',
            xticklabels=[f'Pos{i}' for i in range(head_0_attn.shape[1])],
            yticklabels=[f'Pos{i}' for i in range(head_0_attn.shape[0])]
        )
        plt.title('è§£ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡ (ç¬¬1å±‚, ç¬¬1å¤´)')
        plt.xlabel('é”®ä½ç½®')
        plt.ylabel('æŸ¥è¯¢ä½ç½®')
        plt.tight_layout()
        plt.savefig('decoder_self_attention.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("âœ… è§£ç å™¨è‡ªæ³¨æ„åŠ›å¯è§†åŒ–å·²ä¿å­˜ä¸º 'decoder_self_attention.png'")
    
    print()


def example7_device_usage():
    """ç¤ºä¾‹7ï¼šè®¾å¤‡ä½¿ç”¨ï¼ˆCPU/GPUï¼‰"""
    print("=== ç¤ºä¾‹7ï¼šè®¾å¤‡ä½¿ç”¨ ===")
    
    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    device = get_device()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    config = get_config('small')
    model = Transformer(config).to(device)
    
    # åˆ›å»ºæ•°æ®å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
    src = torch.randint(1, config.vocab_size, (2, 8)).to(device)
    tgt = torch.randint(1, config.vocab_size, (2, 6)).to(device)
    
    print(f"æ¨¡å‹è®¾å¤‡: {next(model.parameters()).device}")
    print(f"æ•°æ®è®¾å¤‡: {src.device}")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        if device.type == 'cuda':
            # ä½¿ç”¨æ··åˆç²¾åº¦
            with torch.cuda.amp.autocast():
                output = model(src, tgt)
        else:
            output = model(src, tgt)
    
    print(f"âœ… è¾“å‡ºè®¾å¤‡: {output['logits'].device}")
    print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output['logits'].shape}")
    
    # æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ
    if device.type == 'cuda':
        memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB
        memory_reserved = torch.cuda.memory_reserved() / 1024**2   # MB
        print(f"GPU å†…å­˜ä½¿ç”¨: {memory_allocated:.1f} MB (å·²åˆ†é…)")
        print(f"GPU å†…å­˜ä¿ç•™: {memory_reserved:.1f} MB (å·²ä¿ç•™)")
    
    print()


def run_all_examples():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ Transformer-PyTorch åŸºç¡€ä½¿ç”¨ç¤ºä¾‹\n")
    
    try:
        example1_basic_components()
        example2_model_configurations()
        example3_complete_model()
        example4_encoder_decoder_separate()
        example5_text_generation()
        example6_attention_visualization()
        example7_device_usage()
        
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ’¡ æç¤º:")
        print("- è¿™äº›ç¤ºä¾‹å±•ç¤ºäº† Transformer-PyTorch çš„ä¸»è¦åŠŸèƒ½")
        print("- å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹é…ç½®å’Œå‚æ•°")
        print("- æŸ¥çœ‹ç”Ÿæˆçš„æ³¨æ„åŠ›å¯è§†åŒ–å›¾ç‰‡")
        print("- åœ¨ GPU ä¸Šè¿è¡Œå¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    run_all_examples()
