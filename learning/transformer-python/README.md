# Transformer-PyTorch

> åŸºäº PyTorch çš„ Transformer æ¶æ„å®ç°ï¼Œä¸“ä¸ºå­¦ä¹ å’Œç ”ç©¶è®¾è®¡
>
> **ğŸ†• ç¬¬ä¸‰ç« æ‰©å±•ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æ”¯æŒ**

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://python.org)
[![PyTorch Version](https://img.shields.io/badge/pytorch-1.9%2B-orange.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## ğŸ“– é¡¹ç›®ç®€ä»‹

Transformer-PyTorch æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Transformer æ¶æ„å®ç°ï¼ŒåŸºäºã€Šç¬¬äºŒç«  Transformeræ¶æ„ã€‹å’Œã€Šç¬¬ä¸‰ç«  é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‹æ–‡æ¡£ä¸­çš„ç†è®ºçŸ¥è¯†ï¼Œä½¿ç”¨ PyTorch æ¡†æ¶æ„å»ºã€‚æœ¬é¡¹ç›®æä¾›äº†æ¨¡å—åŒ–ã€æ˜“äºç†è§£å’Œä½¿ç”¨çš„ Transformer ç»„ä»¶ï¼Œç°å·²æ‰©å±•æ”¯æŒ BERTã€GPTã€T5 ç­‰ä¸»æµé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒ GPU åŠ é€Ÿå’Œç°ä»£æ·±åº¦å­¦ä¹ æœ€ä½³å®è·µã€‚

### âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡** - æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å®ç°ï¼Œæ”¯æŒæŒ‰éœ€ä½¿ç”¨å’Œè‡ªå®šä¹‰
- ğŸš€ **PyTorch ä¼˜åŒ–** - å……åˆ†åˆ©ç”¨ PyTorch çš„è‡ªåŠ¨å¾®åˆ†å’Œ GPU åŠ é€Ÿ
- ğŸ“š **è¯¦ç»†æ³¨é‡Š** - æ‰€æœ‰ä»£ç éƒ½æœ‰ä¸­æ–‡æ³¨é‡Šå’Œå®Œæ•´çš„ docstring
- ğŸ”§ **çµæ´»é…ç½®** - æ”¯æŒå¤šç§é¢„è®¾é…ç½®å’Œè‡ªå®šä¹‰å‚æ•°
- ğŸ¯ **æ•™è‚²å‹å¥½** - ä»£ç ç»“æ„æ¸…æ™°ï¼Œä¾¿äºå­¦ä¹ å’Œç†è§£ Transformer åŸç†
- âš¡ **é«˜æ€§èƒ½** - æ”¯æŒ GPU åŠ é€Ÿã€æ··åˆç²¾åº¦è®­ç»ƒå’Œæ‰¹å¤„ç†è®¡ç®—
- ğŸ”¬ **ç ”ç©¶å¯¼å‘** - æ˜“äºæ‰©å±•å’Œä¿®æ”¹ï¼Œé€‚åˆç ”ç©¶å’Œå®éªŒ
- ğŸ¤– **é¢„è®­ç»ƒæ¨¡å‹** - æ”¯æŒ BERTã€GPTã€T5 ç­‰ä¸»æµé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- ğŸ“ **é¢„è®­ç»ƒä»»åŠ¡** - å®ç° MLMã€NSPã€SOPã€CLM ç­‰é¢„è®­ç»ƒä»»åŠ¡

### ğŸ—ï¸ æ¶æ„ç»„ä»¶

#### æ ¸å¿ƒç»„ä»¶
- **æ•°å­¦å·¥å…·** - æ¿€æ´»å‡½æ•°ã€æ³¨æ„åŠ›è®¡ç®—ã€åˆå§‹åŒ–å‡½æ•°
- **åŸºç¡€å±‚** - çº¿æ€§å±‚ã€å±‚å½’ä¸€åŒ–ã€RMSNormã€å‰é¦ˆç½‘ç»œã€Dropout
- **æ³¨æ„åŠ›æœºåˆ¶** - å¤šå¤´æ³¨æ„åŠ›ã€è‡ªæ³¨æ„åŠ›ã€äº¤å‰æ³¨æ„åŠ›ã€æ©ç å·¥å…·
- **åµŒå…¥å±‚** - è¯åµŒå…¥ã€æ­£å¼¦ä½ç½®ç¼–ç ã€å¯å­¦ä¹ ä½ç½®ç¼–ç 
- **ç¼–ç å™¨** - Transformer ç¼–ç å™¨å±‚å’Œç¼–ç å™¨å—
- **è§£ç å™¨** - Transformer è§£ç å™¨å±‚å’Œè§£ç å™¨å—
- **å®Œæ•´æ¨¡å‹** - ç«¯åˆ°ç«¯çš„ Transformer æ¨¡å‹ï¼Œæ”¯æŒå¤šç§ä»»åŠ¡

#### é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
- **BERT ç³»åˆ—** - BERTã€RoBERTaã€ALBERT ç­‰åŒå‘è¯­è¨€æ¨¡å‹
- **GPT ç³»åˆ—** - GPTã€GPT-2 ç­‰è‡ªå›å½’è¯­è¨€æ¨¡å‹
- **T5 æ¨¡å‹** - æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢çš„ Encoder-Decoder æ¨¡å‹
- **åˆ†ç±»å¤´** - åºåˆ—åˆ†ç±»ã€Tokenåˆ†ç±»ã€è¯­è¨€å»ºæ¨¡ã€é—®ç­”ç­‰ä»»åŠ¡å¤´
- **é¢„è®­ç»ƒä»»åŠ¡** - MLMã€NSPã€SOPã€CLM ç­‰é¢„è®­ç»ƒä»»åŠ¡å®ç°

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# ä»æºç å®‰è£…
git clone https://github.com/transformer-pytorch/transformer-pytorch.git
cd transformer-pytorch
pip install -e .

# æˆ–è€…ç›´æ¥å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨

```python
import torch
from transformer_pytorch import TransformerConfig, Transformer

# åˆ›å»ºé…ç½®
config = TransformerConfig(
    vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048,
    max_seq_len=512
)

# åˆ›å»ºæ¨¡å‹
model = Transformer(config)

# å‡†å¤‡æ•°æ®
src = torch.randint(0, 10000, (2, 10))  # [batch_size, src_len]
tgt = torch.randint(0, 10000, (2, 8))   # [batch_size, tgt_len]

# å‰å‘ä¼ æ’­
output = model(src, tgt)
print(f"è¾“å‡ºå½¢çŠ¶: {output['logits'].shape}")  # [2, 8, 10000]
```

### ä½¿ç”¨é¢„è®¾é…ç½®

```python
from transformer_pytorch.config import get_config

# ä½¿ç”¨é¢„è®¾é…ç½®
config = get_config('small')  # 'small', 'default', 'large'
model = Transformer(config)

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
from transformer_pytorch import print_model_info
print_model_info(model)
```

### é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä½¿ç”¨

```python
from transformer_pytorch.models import ModelFactory

# åˆ›å»ºBERTåˆ†ç±»æ¨¡å‹
bert_model = ModelFactory.create_bert(
    model_name='bert-base',
    task_type='classification',
    num_labels=2
)

# åˆ›å»ºGPTè¯­è¨€æ¨¡å‹
gpt_model = ModelFactory.create_gpt(
    model_name='gpt2',
    task_type='causal_lm'
)

# åˆ›å»ºT5æ¡ä»¶ç”Ÿæˆæ¨¡å‹
t5_model = ModelFactory.create_t5(
    model_name='t5-base',
    task_type='conditional_generation'
)

# BERTæ–‡æœ¬åˆ†ç±»
input_ids = torch.randint(0, 1000, (2, 16))
outputs = bert_model(input_ids=input_ids)
predictions = torch.argmax(outputs['logits'], dim=-1)

# GPTæ–‡æœ¬ç”Ÿæˆ
prompt = torch.randint(0, 1000, (1, 5))
generated = gpt_model.generate(
    input_ids=prompt,
    max_length=20,
    temperature=0.8,
    do_sample=True
)

# T5æ–‡æœ¬è½¬æ¢
src_ids = torch.randint(0, 1000, (1, 10))
generated = t5_model.generate(
    input_ids=src_ids,
    max_length=15
)
```

### GPU åŠ é€Ÿ

```python
from transformer_pytorch import get_device

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
device = get_device()
model = model.to(device)
src = src.to(device)
tgt = tgt.to(device)

# å‰å‘ä¼ æ’­
with torch.cuda.amp.autocast():  # æ··åˆç²¾åº¦
    output = model(src, tgt)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
transformer-pytorch/
â”œâ”€â”€ transformer_pytorch/           # ä¸»åŒ…
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒç»„ä»¶
â”‚   â”‚   â”œâ”€â”€ math_utils.py         # æ•°å­¦å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ layers.py             # åŸºç¡€ç¥ç»ç½‘ç»œå±‚
â”‚   â”‚   â”œâ”€â”€ attention.py          # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ embedding.py          # åµŒå…¥å±‚
â”‚   â”‚   â”œâ”€â”€ encoder.py            # ç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ decoder.py            # è§£ç å™¨
â”‚   â”‚   â””â”€â”€ transformer.py        # å®Œæ•´æ¨¡å‹
â”‚   â”œâ”€â”€ config/                   # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ config.py             # æ¨¡å‹é…ç½®
â”‚   â”‚   â””â”€â”€ constants.py          # å¸¸é‡å®šä¹‰
â”‚   â””â”€â”€ __init__.py               # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ examples/                     # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/                        # å•å…ƒæµ‹è¯•
â”œâ”€â”€ notebooks/                    # Jupyter æ•™ç¨‹
â”œâ”€â”€ docs/                         # è¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ setup.py                      # å®‰è£…è„šæœ¬
â”œâ”€â”€ requirements.txt              # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
```

## ğŸ”§ é…ç½®é€‰é¡¹

### é¢„è®¾é…ç½®

| é…ç½®åç§° | è¯æ±‡è¡¨ | æ¨¡å‹ç»´åº¦ | å±‚æ•° | æ³¨æ„åŠ›å¤´æ•° | æœ€å¤§åºåˆ—é•¿åº¦ | å‚æ•°é‡ |
|----------|--------|----------|------|------------|--------------|--------|
| `small` | 10,000 | 256 | 4 | 4 | 128 | ~11M |
| `default` | 30,000 | 512 | 6 | 8 | 512 | ~65M |
| `large` | 50,000 | 1024 | 12 | 16 | 1024 | ~355M |

### è‡ªå®šä¹‰é…ç½®

```python
from transformer_pytorch.config import create_config, get_config

# åŸºäºé»˜è®¤é…ç½®åˆ›å»ºè‡ªå®šä¹‰é…ç½®
custom_config = create_config(
    base_config=get_config('default'),
    vocab_size=8000,
    d_model=256,
    num_encoder_layers=4,
    num_decoder_layers=4,
    activation='gelu',
    dropout=0.05
)
```

## ğŸ“š è¯¦ç»†æ•™ç¨‹

### Jupyter Notebooks

- [01_åŸºç¡€æ¦‚å¿µ.ipynb](notebooks/01_basic_concepts.ipynb) - Transformer åŸºç¡€æ¦‚å¿µ
- [02_æ³¨æ„åŠ›æœºåˆ¶.ipynb](notebooks/02_attention_mechanism.ipynb) - æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£
- [03_å®Œæ•´æ¨¡å‹.ipynb](notebooks/03_complete_model.ipynb) - å®Œæ•´æ¨¡å‹ä½¿ç”¨
- [04_è®­ç»ƒç¤ºä¾‹.ipynb](notebooks/04_training_example.ipynb) - è®­ç»ƒç¤ºä¾‹
- [05_å¯è§†åŒ–åˆ†æ.ipynb](notebooks/05_visualization.ipynb) - æ³¨æ„åŠ›å¯è§†åŒ–

### ä»£ç ç¤ºä¾‹

```python
# å•ç‹¬ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
from transformer_pytorch.core import MultiHeadAttention

attention = MultiHeadAttention(d_model=512, num_heads=8)
x = torch.randn(2, 10, 512)
output, weights = attention(x, x, x)

# å•ç‹¬ä½¿ç”¨ç¼–ç å™¨
from transformer_pytorch.core import create_encoder

encoder = create_encoder(
    d_model=512, num_heads=8, d_ff=2048, num_layers=6
)
encoded = encoder(x)

# æ–‡æœ¬ç”Ÿæˆ
model.eval()
generated = model.generate(
    src=src,
    max_length=50,
    temperature=0.8,
    do_sample=True
)
```

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_attention.py -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=transformer_pytorch --cov-report=html
```

### æ€§èƒ½åŸºå‡†

```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
python examples/benchmark.py

# å†…å­˜ä½¿ç”¨åˆ†æ
python examples/memory_analysis.py
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### æ•™è‚²å­¦ä¹ 
- ç†è§£ Transformer æ¶æ„åŸç†
- å­¦ä¹ æ³¨æ„åŠ›æœºåˆ¶å®ç°
- æ·±åº¦å­¦ä¹ æ¦‚å¿µå®è·µ

### ç ”ç©¶å¼€å‘
- å¿«é€ŸåŸå‹éªŒè¯
- ç®—æ³•æ”¹è¿›å’Œå®éªŒ
- æ–°æ¶æ„æ¢ç´¢

### å®é™…åº”ç”¨
- æœºå™¨ç¿»è¯‘
- æ–‡æœ¬æ‘˜è¦
- é—®ç­”ç³»ç»Ÿ
- è¯­è¨€å»ºæ¨¡

## âš¡ æ€§èƒ½ä¼˜åŒ–

### GPU åŠ é€Ÿ
```python
# ä½¿ç”¨ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# æ··åˆç²¾åº¦è®­ç»ƒ
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    output = model(src, tgt)
```

### å†…å­˜ä¼˜åŒ–
```python
# æ¢¯åº¦æ£€æŸ¥ç‚¹
from torch.utils.checkpoint import checkpoint

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
output = checkpoint(layer, x)
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

1. **æŠ¥å‘Šé—®é¢˜** - åœ¨ Issues ä¸­æŠ¥å‘Š bug æˆ–æå‡ºåŠŸèƒ½è¯·æ±‚
2. **ä»£ç è´¡çŒ®** - Fork é¡¹ç›®ï¼Œåˆ›å»ºåˆ†æ”¯ï¼Œæäº¤ Pull Request
3. **æ–‡æ¡£æ”¹è¿›** - æ”¹è¿›æ–‡æ¡£ã€æ·»åŠ ç¤ºä¾‹ã€ä¿®æ­£é”™è¯¯
4. **æµ‹è¯•ç”¨ä¾‹** - æ·»åŠ æµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜ä»£ç è¦†ç›–ç‡

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/transformer-pytorch/transformer-pytorch.git
cd transformer-pytorch

# å®‰è£…å¼€å‘ä¾èµ–
pip install -e ".[dev]"

# å®‰è£… pre-commit é’©å­
pre-commit install

# è¿è¡Œä»£ç æ ¼å¼åŒ–
black transformer_pytorch/
isort transformer_pytorch/

# è¿è¡Œç±»å‹æ£€æŸ¥
mypy transformer_pytorch/
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- æ„Ÿè°¢ Vaswani ç­‰äººçš„å¼€åˆ›æ€§è®ºæ–‡ "Attention Is All You Need"
- æ„Ÿè°¢ã€Šç¬¬äºŒç«  Transformeræ¶æ„ã€‹æ–‡æ¡£æä¾›çš„ç†è®ºåŸºç¡€
- æ„Ÿè°¢ PyTorch å›¢é˜Ÿæä¾›çš„ä¼˜ç§€æ·±åº¦å­¦ä¹ æ¡†æ¶
- æ„Ÿè°¢æ‰€æœ‰ä¸ºå¼€æºç¤¾åŒºåšå‡ºè´¡çŒ®çš„å¼€å‘è€…

## ğŸ“ è”ç³»æ–¹å¼

- **GitHub Issues**: [æäº¤é—®é¢˜](https://github.com/transformer-pytorch/transformer-pytorch/issues)
- **é‚®ä»¶**: transformer-pytorch@example.com
- **æ–‡æ¡£**: [åœ¨çº¿æ–‡æ¡£](https://transformer-pytorch.readthedocs.io/)

---

**Transformer-PyTorch** - è®© Transformer æ¶æ„çš„å­¦ä¹ å’Œç ”ç©¶å˜å¾—ç®€å•é«˜æ•ˆï¼
