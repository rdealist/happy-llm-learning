"""
Transformer-PyTorch: åŸºäº PyTorch çš„ Transformer æ¶æ„å®ç°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Transformer å®ç°ï¼ŒåŸºäºã€Šç¬¬äºŒç«  Transformeræ¶æ„ã€‹æ–‡æ¡£ï¼Œ
æä¾›äº†æ¨¡å—åŒ–ã€æ˜“äºç†è§£å’Œä½¿ç”¨çš„ Transformer ç»„ä»¶ã€‚

ä¸»è¦ç‰¹æ€§:
- ğŸ§© æ¨¡å—åŒ–è®¾è®¡ - æ¯ä¸ªç»„ä»¶ç‹¬ç«‹å®ç°ï¼Œæ”¯æŒæŒ‰éœ€ä½¿ç”¨
- ğŸš€ PyTorch ä¼˜åŒ– - å……åˆ†åˆ©ç”¨ PyTorch çš„è‡ªåŠ¨å¾®åˆ†å’Œ GPU åŠ é€Ÿ
- ğŸ“š è¯¦ç»†æ³¨é‡Š - æ‰€æœ‰ä»£ç éƒ½æœ‰ä¸­æ–‡æ³¨é‡Šå’Œè¯¦ç»†æ–‡æ¡£
- ğŸ”§ çµæ´»é…ç½® - æ”¯æŒå¤šç§é¢„è®¾é…ç½®å’Œè‡ªå®šä¹‰å‚æ•°
- ğŸ¯ æ•™è‚²å‹å¥½ - ä»£ç ç»“æ„æ¸…æ™°ï¼Œä¾¿äºå­¦ä¹ å’Œç†è§£
- âš¡ é«˜æ€§èƒ½ - æ”¯æŒ GPU åŠ é€Ÿå’Œæ‰¹å¤„ç†è®¡ç®—

ä½œè€…: shihom_wu
ç‰ˆæœ¬: 1.0.0
è®¸å¯: MIT License
"""

__version__ = "1.0.0"
__author__ = "shihom_wu"
__email__ = "transformer-pytorch@example.com"
__license__ = "MIT"

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from .core.math_utils import *
from .core.layers import *
from .core.attention import *
from .core.embedding import *
from .core.encoder import *
from .core.decoder import *
from .core.transformer import *

# å¯¼å…¥é…ç½®
from .config.config import *
from .config.constants import *

# ç‰ˆæœ¬ä¿¡æ¯
VERSION_INFO = {
    'major': 1,
    'minor': 0,
    'patch': 0,
    'version': __version__,
    'pytorch_version': None,  # å°†åœ¨è¿è¡Œæ—¶æ£€æµ‹
    'cuda_available': None,   # å°†åœ¨è¿è¡Œæ—¶æ£€æµ‹
}

def get_version_info():
    """
    è·å–ç‰ˆæœ¬ä¿¡æ¯å’Œç¯å¢ƒä¿¡æ¯
    
    Returns:
        dict: åŒ…å«ç‰ˆæœ¬å’Œç¯å¢ƒä¿¡æ¯çš„å­—å…¸
    """
    import torch
    
    VERSION_INFO['pytorch_version'] = torch.__version__
    VERSION_INFO['cuda_available'] = torch.cuda.is_available()
    
    if torch.cuda.is_available():
        VERSION_INFO['cuda_version'] = torch.version.cuda
        VERSION_INFO['gpu_count'] = torch.cuda.device_count()
        VERSION_INFO['current_device'] = torch.cuda.current_device()
    
    return VERSION_INFO

def print_version_info():
    """æ‰“å°ç‰ˆæœ¬ä¿¡æ¯"""
    info = get_version_info()
    print(f"Transformer-PyTorch v{info['version']}")
    print(f"PyTorch v{info['pytorch_version']}")
    print(f"CUDA Available: {info['cuda_available']}")
    
    if info['cuda_available']:
        print(f"CUDA Version: {info.get('cuda_version', 'Unknown')}")
        print(f"GPU Count: {info.get('gpu_count', 0)}")

# è®¾ç½®é»˜è®¤çš„éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
def set_seed(seed: int = 42):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
    
    Args:
        seed (int): éšæœºç§å­å€¼
    """
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ç¡®ä¿ CUDA æ“ä½œçš„ç¡®å®šæ€§
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

# è‡ªåŠ¨æ£€æµ‹å’Œè®¾ç½®è®¾å¤‡
def get_device(prefer_gpu: bool = True) -> torch.device:
    """
    è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³å¯ç”¨è®¾å¤‡
    
    Args:
        prefer_gpu (bool): æ˜¯å¦ä¼˜å…ˆä½¿ç”¨ GPU
        
    Returns:
        torch.device: è®¾å¤‡å¯¹è±¡
    """
    import torch
    
    if prefer_gpu and torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ä½¿ç”¨ GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device('cpu')
        print("ä½¿ç”¨ CPU")
    
    return device

# å†…å­˜ä½¿ç”¨ç›‘æ§
def get_memory_usage():
    """
    è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
    
    Returns:
        dict: å†…å­˜ä½¿ç”¨ä¿¡æ¯
    """
    import torch
    import psutil
    
    memory_info = {
        'cpu_memory_percent': psutil.virtual_memory().percent,
        'cpu_memory_available_gb': psutil.virtual_memory().available / (1024**3),
    }
    
    if torch.cuda.is_available():
        memory_info.update({
            'gpu_memory_allocated_gb': torch.cuda.memory_allocated() / (1024**3),
            'gpu_memory_reserved_gb': torch.cuda.memory_reserved() / (1024**3),
            'gpu_memory_total_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3),
        })
    
    return memory_info

def print_memory_usage():
    """æ‰“å°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    memory = get_memory_usage()
    print(f"CPU å†…å­˜ä½¿ç”¨: {memory['cpu_memory_percent']:.1f}%")
    print(f"CPU å¯ç”¨å†…å­˜: {memory['cpu_memory_available_gb']:.2f} GB")
    
    if 'gpu_memory_allocated_gb' in memory:
        print(f"GPU å·²åˆ†é…å†…å­˜: {memory['gpu_memory_allocated_gb']:.2f} GB")
        print(f"GPU ä¿ç•™å†…å­˜: {memory['gpu_memory_reserved_gb']:.2f} GB")
        print(f"GPU æ€»å†…å­˜: {memory['gpu_memory_total_gb']:.2f} GB")

# æ¨¡å‹å¤§å°è®¡ç®—
def count_parameters(model) -> dict:
    """
    è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    
    Args:
        model: PyTorch æ¨¡å‹
        
    Returns:
        dict: å‚æ•°ç»Ÿè®¡ä¿¡æ¯
    """
    import torch.nn as nn
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return {
        'total_parameters': total_params,
        'trainable_parameters': trainable_params,
        'non_trainable_parameters': total_params - trainable_params,
        'total_parameters_M': f"{total_params / 1e6:.2f}M",
        'trainable_parameters_M': f"{trainable_params / 1e6:.2f}M",
    }

def print_model_info(model):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    params = count_parameters(model)
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {params['total_parameters_M']}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {params['trainable_parameters_M']}")
    print(f"  ä¸å¯è®­ç»ƒå‚æ•°: {params['non_trainable_parameters']:,}")

# å¯¼å‡ºçš„å…¬å…±æ¥å£
__all__ = [
    # ç‰ˆæœ¬å’Œç¯å¢ƒ
    '__version__',
    'get_version_info',
    'print_version_info',
    'set_seed',
    'get_device',
    
    # å†…å­˜å’Œæ¨¡å‹å·¥å…·
    'get_memory_usage',
    'print_memory_usage',
    'count_parameters',
    'print_model_info',
    
    # æ ¸å¿ƒç»„ä»¶ (å°†åœ¨å„æ¨¡å—ä¸­å®šä¹‰)
    'TransformerConfig',
    'Transformer',
    'TransformerEncoder',
    'TransformerDecoder',
    'MultiHeadAttention',
    'TransformerEmbedding',
    'PositionalEncoding',
]

# åˆå§‹åŒ–æ—¶çš„æ¬¢è¿ä¿¡æ¯
def _welcome_message():
    """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
    print("ğŸš€ æ¬¢è¿ä½¿ç”¨ Transformer-PyTorch!")
    print("ğŸ“š åŸºäºã€Šç¬¬äºŒç«  Transformeræ¶æ„ã€‹çš„å®Œæ•´å®ç°")
    print("ğŸ’¡ ä½¿ç”¨ help(transformer_pytorch) æŸ¥çœ‹æ›´å¤šä¿¡æ¯")

# å¯é€‰çš„æ¬¢è¿ä¿¡æ¯ï¼ˆä»…åœ¨äº¤äº’å¼ç¯å¢ƒä¸­æ˜¾ç¤ºï¼‰
import sys
if hasattr(sys, 'ps1'):  # æ£€æŸ¥æ˜¯å¦åœ¨äº¤äº’å¼ç¯å¢ƒä¸­
    _welcome_message()
