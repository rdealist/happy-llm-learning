{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-PyTorch 基础概念教程\n",
    "\n",
    "本教程将介绍 Transformer 架构的基础概念，并演示如何使用 Transformer-PyTorch 库。\n",
    "\n",
    "## 目录\n",
    "1. [环境设置](#环境设置)\n",
    "2. [Transformer 架构概述](#transformer-架构概述)\n",
    "3. [注意力机制](#注意力机制)\n",
    "4. [位置编码](#位置编码)\n",
    "5. [编码器和解码器](#编码器和解码器)\n",
    "6. [完整模型](#完整模型)\n",
    "7. [实际应用](#实际应用)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境设置\n",
    "\n",
    "首先导入必要的库并设置环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 导入 Transformer-PyTorch\n",
    "from transformer_pytorch import (\n",
    "    TransformerConfig,\n",
    "    Transformer,\n",
    "    get_config,\n",
    "    set_seed,\n",
    "    get_device,\n",
    "    print_model_info\n",
    ")\n",
    "from transformer_pytorch.core import (\n",
    "    MultiHeadAttention,\n",
    "    TransformerEmbedding,\n",
    "    create_encoder,\n",
    "    create_decoder\n",
    ")\n",
    "\n",
    "# 设置随机种子和设备\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 设置绘图样式\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 架构概述\n",
    "\n",
    "Transformer 是一种基于注意力机制的神经网络架构，由编码器和解码器组成。\n",
    "\n",
    "### 主要组件：\n",
    "- **多头注意力机制** (Multi-Head Attention)\n",
    "- **位置编码** (Positional Encoding)\n",
    "- **前馈网络** (Feed Forward Network)\n",
    "- **层归一化** (Layer Normalization)\n",
    "- **残差连接** (Residual Connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个小型 Transformer 配置\n",
    "config = TransformerConfig(\n",
    "    vocab_size=1000,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    d_ff=256,\n",
    "    max_seq_len=32,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Transformer 配置:\")\n",
    "print(f\"词汇表大小: {config.vocab_size:,}\")\n",
    "print(f\"模型维度: {config.d_model}\")\n",
    "print(f\"注意力头数: {config.num_heads}\")\n",
    "print(f\"编码器层数: {config.num_encoder_layers}\")\n",
    "print(f\"解码器层数: {config.num_decoder_layers}\")\n",
    "\n",
    "# 估算参数量\n",
    "params = config.estimate_parameters()\n",
    "print(f\"\\n估算参数量: {params['total_M']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "\n",
    "注意力机制是 Transformer 的核心，它允许模型在处理序列时关注不同位置的信息。\n",
    "\n",
    "### 缩放点积注意力公式：\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示多头注意力机制\n",
    "d_model, num_heads = 128, 4\n",
    "attention = MultiHeadAttention(d_model, num_heads, dropout=0.0)\n",
    "\n",
    "# 创建示例输入\n",
    "batch_size, seq_len = 2, 8\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "\n",
    "# 前向传播\n",
    "output, attention_weights = attention(x, x, x)\n",
    "\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"参数数量: {sum(p.numel() for p in attention.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化注意力权重\n",
    "# 取第一个样本、第一个头的注意力权重\n",
    "attn_matrix = attention_weights[0, 0].detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    attn_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='Blues',\n",
    "    xticklabels=[f'K{i}' for i in range(seq_len)],\n",
    "    yticklabels=[f'Q{i}' for i in range(seq_len)]\n",
    ")\n",
    "plt.title('多头注意力权重矩阵 (第1个头)')\n",
    "plt.xlabel('键 (Key) 位置')\n",
    "plt.ylabel('查询 (Query) 位置')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 验证注意力权重是否归一化\n",
    "row_sums = attn_matrix.sum(axis=1)\n",
    "print(f\"\\n注意力权重行和: {row_sums}\")\n",
    "print(f\"是否归一化: {np.allclose(row_sums, 1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "\n",
    "由于注意力机制本身不包含位置信息，Transformer 使用位置编码来为序列中的每个位置添加位置信息。\n",
    "\n",
    "### 正弦余弦位置编码公式：\n",
    "- $PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$\n",
    "- $PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示位置编码\n",
    "vocab_size, d_model, max_len = 1000, 128, 32\n",
    "embedding = TransformerEmbedding(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    max_len=max_len,\n",
    "    position_encoding_type='sinusoidal',\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# 创建示例词元序列\n",
    "token_ids = torch.randint(1, vocab_size, (1, 16))\n",
    "embedded = embedding(token_ids)\n",
    "\n",
    "print(f\"词元ID: {token_ids[0][:8].tolist()}...\")\n",
    "print(f\"嵌入输出形状: {embedded.shape}\")\n",
    "\n",
    "# 提取位置编码\n",
    "pos_encoding = embedding.position_encoding.encoding.pe[0, :16, :].detach().numpy()\n",
    "\n",
    "# 可视化位置编码\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 显示前64个维度的位置编码\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(\n",
    "    pos_encoding[:, :64].T,\n",
    "    cmap='RdBu_r',\n",
    "    center=0,\n",
    "    xticklabels=range(0, 16, 2),\n",
    "    yticklabels=range(0, 64, 8)\n",
    ")\n",
    "plt.title('正弦余弦位置编码')\n",
    "plt.xlabel('位置')\n",
    "plt.ylabel('维度')\n",
    "\n",
    "# 显示几个维度的位置编码曲线\n",
    "plt.subplot(1, 2, 2)\n",
    "for dim in [0, 1, 2, 3]:\n",
    "    plt.plot(pos_encoding[:, dim], label=f'维度 {dim}')\n",
    "plt.title('位置编码曲线')\n",
    "plt.xlabel('位置')\n",
    "plt.ylabel('编码值')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器和解码器\n",
    "\n",
    "### 编码器\n",
    "编码器由多个相同的层组成，每层包含：\n",
    "1. 多头自注意力机制\n",
    "2. 残差连接和层归一化\n",
    "3. 前馈网络\n",
    "4. 残差连接和层归一化\n",
    "\n",
    "### 解码器\n",
    "解码器也由多个相同的层组成，每层包含：\n",
    "1. 掩码多头自注意力机制\n",
    "2. 残差连接和层归一化\n",
    "3. 多头交叉注意力机制\n",
    "4. 残差连接和层归一化\n",
    "5. 前馈网络\n",
    "6. 残差连接和层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示编码器\n",
    "d_model, num_heads, d_ff, num_layers = 128, 4, 256, 2\n",
    "encoder = create_encoder(d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# 输入数据\n",
    "x = torch.randn(2, 10, d_model)\n",
    "print(f\"编码器输入形状: {x.shape}\")\n",
    "\n",
    "# 编码\n",
    "encoded = encoder(x)\n",
    "print(f\"编码器输出形状: {encoded.shape}\")\n",
    "print(f\"编码器参数数量: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示解码器\n",
    "decoder = create_decoder(d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# 目标序列和编码器输出\n",
    "tgt = torch.randn(2, 8, d_model)\n",
    "memory = encoded  # 使用编码器的输出作为记忆\n",
    "\n",
    "print(f\"解码器输入形状: {tgt.shape}\")\n",
    "print(f\"记忆形状: {memory.shape}\")\n",
    "\n",
    "# 解码\n",
    "decoded = decoder(tgt, memory)\n",
    "print(f\"解码器输出形状: {decoded.shape}\")\n",
    "print(f\"解码器参数数量: {sum(p.numel() for p in decoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整模型\n",
    "\n",
    "现在让我们创建并使用一个完整的 Transformer 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建完整的 Transformer 模型\n",
    "model = Transformer(config).to(device)\n",
    "\n",
    "# 打印模型信息\n",
    "print_model_info(model)\n",
    "\n",
    "# 准备示例数据\n",
    "batch_size = 2\n",
    "src_len, tgt_len = 12, 10\n",
    "\n",
    "src = torch.randint(1, config.vocab_size, (batch_size, src_len)).to(device)\n",
    "tgt = torch.randint(1, config.vocab_size, (batch_size, tgt_len)).to(device)\n",
    "\n",
    "print(f\"\\n输入数据:\")\n",
    "print(f\"源序列形状: {src.shape}\")\n",
    "print(f\"目标序列形状: {tgt.shape}\")\n",
    "print(f\"源序列示例: {src[0][:8].tolist()}...\")\n",
    "print(f\"目标序列示例: {tgt[0][:8].tolist()}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(src, tgt)\n",
    "\n",
    "print(f\"模型输出:\")\n",
    "print(f\"Logits 形状: {output['logits'].shape}\")\n",
    "print(f\"最后隐藏状态形状: {output['last_hidden_state'].shape}\")\n",
    "\n",
    "# 计算损失和困惑度\n",
    "logits = output['logits']\n",
    "shift_logits = logits[..., :-1, :].contiguous()\n",
    "shift_labels = tgt[..., 1:].contiguous()\n",
    "\n",
    "loss = F.cross_entropy(\n",
    "    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "    shift_labels.view(-1),\n",
    "    ignore_index=config.pad_token_id\n",
    ")\n",
    "perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"\\n性能指标:\")\n",
    "print(f\"损失: {loss.item():.4f}\")\n",
    "print(f\"困惑度: {perplexity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际应用\n",
    "\n",
    "让我们演示一些实际的应用场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 序列到序列翻译（模拟）\n",
    "print(\"=== 序列到序列翻译 ===\")\n",
    "\n",
    "# 模拟源语言和目标语言句子\n",
    "src_sentence = torch.tensor([[1, 10, 25, 67, 89, 2]]).to(device)  # \"BOS hello world how are EOS\"\n",
    "tgt_sentence = torch.tensor([[1, 15, 30, 45, 60]]).to(device)     # \"BOS 你好 世界 怎么 样\"\n",
    "\n",
    "print(f\"源句子: {src_sentence[0].tolist()}\")\n",
    "print(f\"目标句子: {tgt_sentence[0].tolist()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    translation_output = model(src_sentence, tgt_sentence)\n",
    "    \n",
    "# 获取每个位置的预测\n",
    "predictions = torch.argmax(translation_output['logits'], dim=-1)\n",
    "print(f\"模型预测: {predictions[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 文本生成\n",
    "print(\"\\n=== 文本生成 ===\")\n",
    "\n",
    "# 使用模型生成文本\n",
    "src_context = torch.tensor([[1, 20, 35, 50, 2]]).to(device)  # 上下文\n",
    "\n",
    "print(f\"输入上下文: {src_context[0].tolist()}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        src=src_context,\n",
    "        max_length=15,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(f\"生成序列: {generated[0].tolist()}\")\n",
    "print(f\"生成长度: {generated.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 注意力分析\n",
    "print(\"\\n=== 注意力分析 ===\")\n",
    "\n",
    "# 获取详细的注意力权重\n",
    "with torch.no_grad():\n",
    "    detailed_output = model(src_sentence, tgt_sentence, return_dict=True)\n",
    "\n",
    "# 分析编码器注意力\n",
    "if detailed_output['encoder_attentions'] is not None:\n",
    "    encoder_attn = detailed_output['encoder_attentions'][0]  # 第一层\n",
    "    print(f\"编码器注意力形状: {encoder_attn.shape}\")\n",
    "    \n",
    "    # 可视化第一个头的注意力\n",
    "    attn_head_0 = encoder_attn[0, 0].cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        attn_head_0,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=[f'Pos{i}' for i in range(attn_head_0.shape[1])],\n",
    "        yticklabels=[f'Pos{i}' for i in range(attn_head_0.shape[0])]\n",
    "    )\n",
    "    plt.title('编码器自注意力权重 (第1层, 第1头)')\n",
    "    plt.xlabel('键位置')\n",
    "    plt.ylabel('查询位置')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 分析解码器交叉注意力\n",
    "if detailed_output['decoder_cross_attentions'] is not None:\n",
    "    cross_attn = detailed_output['decoder_cross_attentions'][0]  # 第一层\n",
    "    print(f\"\\n解码器交叉注意力形状: {cross_attn.shape}\")\n",
    "    \n",
    "    # 可视化第一个头的交叉注意力\n",
    "    cross_attn_head_0 = cross_attn[0, 0].cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cross_attn_head_0,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Reds',\n",
    "        xticklabels=[f'Src{i}' for i in range(cross_attn_head_0.shape[1])],\n",
    "        yticklabels=[f'Tgt{i}' for i in range(cross_attn_head_0.shape[0])]\n",
    "    )\n",
    "    plt.title('解码器交叉注意力权重 (第1层, 第1头)')\n",
    "    plt.xlabel('源序列位置')\n",
    "    plt.ylabel('目标序列位置')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "在本教程中，我们学习了：\n",
    "\n",
    "1. **Transformer 架构的基本组件**：注意力机制、位置编码、编码器、解码器\n",
    "2. **如何使用 Transformer-PyTorch 库**：创建模型、配置参数、前向传播\n",
    "3. **注意力机制的工作原理**：缩放点积注意力、多头注意力\n",
    "4. **位置编码的重要性**：正弦余弦编码为序列添加位置信息\n",
    "5. **实际应用场景**：序列到序列翻译、文本生成、注意力分析\n",
    "\n",
    "### 下一步\n",
    "- 探索更复杂的模型配置\n",
    "- 学习如何训练 Transformer 模型\n",
    "- 了解不同的注意力机制变体\n",
    "- 实现具体的 NLP 任务\n",
    "\n",
    "### 参考资源\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - 原始 Transformer 论文\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - 可视化教程\n",
    "- [Transformer-PyTorch 文档](https://transformer-pytorch.readthedocs.io/) - 详细 API 文档"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
